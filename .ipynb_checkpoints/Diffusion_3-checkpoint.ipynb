{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mean = sqrt(a_hat_t) * x_0\n",
    "variance = sart(1-a_hat_t) * random_noise\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import PIL\n",
    "import math\n",
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x_0, t, betas = torch.linspace(0.0, 1.0, 5)):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    alphas = 1 - betas\n",
    "    alphas_hat = torch.cumprod(alphas, axis=0)\n",
    "    alphas_hat_t = alphas_hat.gather(-1, t).reshape(-1, 1, 1, 1)\n",
    "    \n",
    "    mean = alphas_hat_t.sqrt() * x_0\n",
    "    variance = torch.sqrt(1 - alphas_hat_t) * noise\n",
    "    \n",
    "    return mean + variance, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZmJy3aSZ1Ix573d2MlJXQowLCLQyIUsPdniOJ7rBsgG4XJb04g9ZFA9MhxYvckeKkVmo&usqp=CAU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'racoon.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716613ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab3420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5658d",
   "metadata": {},
   "source": [
    "# Transfer Image to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ce2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ # PIL -> Torch\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(), # from 0 to 1\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)# 0 -> -1 , 1 -> 1\n",
    "    \n",
    "])\n",
    "reverse_transform = transforms.Compose([ # Torch -> PIL\n",
    "    transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "    transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "    transforms.Lambda(lambda t: t * 255.),\n",
    "    transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c565ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801d339",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d020a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(reverse_transform(torch_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74727f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([0,1,2,3,4])\n",
    "batch_images = torch.stack([torch_image] * 5)\n",
    "noisy_images, _ = forward_diffusion(batch_images, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81bd90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in noisy_images:\n",
    "    plt.imshow(reverse_transform(img))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8314c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a668542",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925971db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image() -> PIL.Image.Image:\n",
    "#     url = url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTZmJy3aSZ1Ix573d2MlJXQowLCLQyIUsPdniOJ7rBsgG4XJb04g9ZFA9MhxYvckeKkVmo&usqp=CAU'\n",
    "#     filename = 'racoon.jpg'\n",
    "#     urllib.request.urlretrieve(url, filename)\n",
    "    return PIL.Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4073a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_noise_distribution(noise, predicted_noise):\n",
    "    plt.hist(noise.cpu().numpy().flatten(), density=True, alpha=0.8, label= \"ground truth noise\")\n",
    "    plt.hist(predicted_noise.cpu().numpy().flatten(), density=True, alpha=0.8, label= \"predicted noise\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f58b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_noise_prediction(noise, predicted_noise):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    f, ax = plt.subplots(1, 2, figsize=(5,5))\n",
    "    ax[0].imshow(reverse_transform(noise))\n",
    "    ax[0].set_title(f\"ground truth noise\", fontsize=10)\n",
    "    ax[1].imshow(reverse_transform(predicted_noise))\n",
    "    ax[1].set_title(f\"predicted noise\", fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e82c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel:\n",
    "    def __init__(self, start_schedule=0.0001, end_schedule=0.02, timesteps=300):\n",
    "        super().__init__()\n",
    "        self.start_schedule = start_schedule\n",
    "        self.end_schedule = end_schedule\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(start_schedule, end_schedule, timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_hat = torch.cumprod(self.alphas, axis=0)\n",
    "        \n",
    "    def forward(self, x_0, t, device):\n",
    "        '''\n",
    "        x_0 : (B, C, H, W)\n",
    "        t: (B,)\n",
    "        '''\n",
    "        noise =- torch.randn_like(x_0)\n",
    "        sqrt_alphas_hat_t = self.get_index_from_list(self.alphas_hat.sqrt(), t, x_0.shape)\n",
    "        sqrt_one_minus_alpha_hat_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_hat), t, x_0.shape)\n",
    "        \n",
    "        mean = sqrt_alphas_hat_t.to(device) * x_0.to(device)\n",
    "        variance = sqrt_one_minus_alpha_hat_t.to(device) * noise.to(device)\n",
    "        \n",
    "        return mean + variance, noise.to(device)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def backward(self, x, t, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the model to predict the noise in the image and returns the denoised image.\n",
    "        Applies noise to this image, if we are not the last step yet.\n",
    "        \"\"\"\n",
    "\n",
    "        betas_t = self.get_index_from_list(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_hat_t = self.get_index_from_list(torch.sqrt(1. - self.alphas_hat), t, x.shape)\n",
    "        sqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.alphas), t, x.shape)\n",
    "        mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) / sqrt_one_minus_alphas_hat_t)\n",
    "\n",
    "        posterior_variance_t = betas_t\n",
    "        \n",
    "        if t == 0:\n",
    "            return mean\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "            variance = torch.sqrt(posterior_variance_t) * noise\n",
    "            return mean + variance\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_index_from_list(values, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        '''\n",
    "        pick the values from vals according to the indices stored in 't'\n",
    "        '''\n",
    "        result = values.gather(-1, t.cpu())\n",
    "        \n",
    "        '''\n",
    "        if x_shape = (5,3,64,64)\n",
    "            -> len(x_shape) = 4\n",
    "            -> len(x_shape) - 1 = 3\n",
    "        and thus we reshape `out` to dim (batch_size, 1, 1, 1)\n",
    "        '''\n",
    "        return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE= (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ # PIL -> Torch\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(), # from 0 to 1\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)# 0 -> -1 , 1 -> 1\n",
    "    \n",
    "])\n",
    "reverse_transform = transforms.Compose([ # Torch -> PIL\n",
    "    transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "    transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "    transforms.Lambda(lambda t: t * 255.),\n",
    "    transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390caeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image = get_sample_image()\n",
    "torch_image = transform(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = DiffusionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self,dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self,time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(),embeddings.cos()),dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, time_embedding_dims, labels, num_filters=3, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        self.time_embedding = SinusoidalPositionalEmbedding(time_embedding_dims)\n",
    "        self.labels = labels\n",
    "        \n",
    "        if labels:\n",
    "            self.label_mlp = nn.Linear(1, channels_out)\n",
    "            \n",
    "        self.downsample = downsample\n",
    "        \n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(channels_in, channels_out, num_filters, padding=1)\n",
    "            self.final = nn.Conv2d(channels_out, channels_out, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(2*channels_in, channels_out, num_filters, padding=1)\n",
    "            self.final = nn.ConvTranspose2d(channels_out, channels_out, 4, 2, 1)\n",
    "        \n",
    "        self.bnorm1 = nn.BatchNorm2d(channels_out)\n",
    "        self.bnorm2 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_embedding_dims, channels_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, **kwargs):\n",
    "        o = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        o_time = self.relu(self.time_mlp(self.time_embedding(t)))\n",
    "        o = o + o_time[(...,) + (None,) * 2]\n",
    "        if self.labels:\n",
    "            label = kwargs.get('labels')\n",
    "            o_label = self.relu(self.label_mlp(label))\n",
    "            o = o + o_label[(...,) + (None,) * 2]\n",
    "        \n",
    "        o = self.bnorm2(self.relu(self.conv2(o)))\n",
    "        return self.final(o)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    '''\n",
    "    A Simplified variant of the U-Net architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, img_channels=3, time_embedding_dims=128, labels=False, sequence_channels=(64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        sequence_channels_rev = reversed(sequence_channels)\n",
    "        \n",
    "        self.downsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels) for channels_in, channels_out in zip(sequence_channels,sequence_channels[1:])])\n",
    "        self.upsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels, downsample=False) for channels_in, channels_out in zip(sequence_channels[::-1],sequence_channels[::-1][1:])])\n",
    "        self.conv1 = nn.Conv2d(img_channels, sequence_channels[0], 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(sequence_channels[0], img_channels, 1)\n",
    "        \n",
    "    def forward(self, x, t, **kwargs):\n",
    "        residuals = []\n",
    "        o = self.conv1(x)\n",
    "        \n",
    "        for ds in self.downsampling:\n",
    "            o = ds(o, t, **kwargs)\n",
    "            residuals.append(o)\n",
    "            \n",
    "        for us, res in zip(self.upsampling, reversed(residuals)):\n",
    "            o = us(torch.cat((o, res), dim=1), t, **kwargs)\n",
    "            \n",
    "        return self.conv2(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_EPOCHS = 2000\n",
    "PRINT_FREQUENCY = 400\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = True\n",
    "\n",
    "unet = UNet(labels=False)\n",
    "unet.to(device)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de39fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NO_EPOCHS):\n",
    "    mse_epoch_loss = []\n",
    "    \n",
    "    batch = torch.stack([torch_image] * BATCH_SIZE)\n",
    "    t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
    "    \n",
    "    batch_noisy, noise = diffusion_model.forward(batch, t, device)\n",
    "    predicted_noise = unet(batch_noisy, t)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = torch.nn.functional.mse_loss(noise, predicted_noise)\n",
    "    mse_epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % PRINT_FREQUENCY == 0:\n",
    "        print('---')\n",
    "        print(f\"Epoch: {epoch} | Train Loss {np.mean(mse_epoch_loss)}\")\n",
    "        if VERBOSE:\n",
    "            with torch.no_grad():        \n",
    "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
    "                plot_noise_distribution(noise, predicted_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse diffusion\n",
    "with torch.no_grad():\n",
    "    img = torch.randn((1, 3) + IMAGE_SHAPE).to(device) # x_300\n",
    "    print(img)\n",
    "    for i in reversed(range(diffusion_model.timesteps)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        img = diffusion_model.backward(img, t, unet.eval())\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            plt.figure(figsize=(2, 2))\n",
    "            plt.imshow(reverse_transform(img[0]))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50edf71",
   "metadata": {},
   "source": [
    "# With Label - Training on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NO_EPOCHS = 100\n",
    "PRINT_FREQUENCY = 10\n",
    "LR = 0.001\n",
    "VERBOSE = True\n",
    "\n",
    "unet = UNet(labels=True)\n",
    "unet.to(device)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3122195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "def minus_one_to_one(x):\n",
    "    return (x * 2) - 1\n",
    "data_transform = [ # PIL -> Torch\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(), # from 0 to 1\n",
    "    transforms.Lambda(minus_one_to_one)# 0 -> -1 , 1 -> 1    \n",
    "]\n",
    "\n",
    "reverse_transform_dataset = transforms.Compose([ # Torch -> PIL\n",
    "    transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "    transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "    transforms.Lambda(lambda t: t * 255.),\n",
    "    transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested_classes = ['bird', 'cat', 'dog', 'horse']\n",
    "# 将类别名转换为对应的索引\n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "interested_class_indices = [cifar10_classes.index(cls) for cls in interested_classes]\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=data_transform)\n",
    "\n",
    "train_indices = [i for i in range(len(cifar10_train)) if cifar10_train.targets[i] in interested_class_indices]\n",
    "test_indices = [i for i in range(len(cifar10_test)) if cifar10_test.targets[i] in interested_class_indices]\n",
    "\n",
    "trainset = torch.utils.data.Subset(cifar10_train, train_indices)\n",
    "testset = torch.utils.data.Subset(cifar10_test, test_indices)\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a80f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = next(iter(trainloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ec8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NO_EPOCHS):\n",
    "    mean_epoch_loss = []\n",
    "    mean_epoch_loss_val = []\n",
    "    \n",
    "    print(f\"EPOCH {epoch} Train\")\n",
    "    for batch, label in trainloader:\n",
    "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        batch_noisy, noise = diffusion_model.forward(batch, t, device)\n",
    "        predicted_noise = unet(batch_noisy, t, labels=label.reshape(-1, 1).float().to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise)\n",
    "        mean_epoch_loss.append(loss)\n",
    "        optimizer.step()\n",
    "    print(f\"EPOCH {epoch} Test\")\n",
    "    for batch, label in testloader:\n",
    "        t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long().to(device)\n",
    "        batch = batch.to(device)\n",
    "        batch_noisy, noise = diffusion_model(batch_noisy, t, labels=label.reshape(-1, 1).float().to(device))\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise)\n",
    "        mean_epoch_loss_val.append(loss.item())\n",
    "        \n",
    "    if epoch % PRINT_FREQUENCY == 0:\n",
    "        print('---')\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {np.mean(mean_epoch_loss)} | Val Loss: {np.mean(mean_epoch_loss_val)}\")\n",
    "        if VERBOSE:\n",
    "            with torch.no_grad():\n",
    "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
    "                plot_noise_distribution(noise, predicted_noise)\n",
    "        \n",
    "        torch.save(unet.state_dict(), f\"epoch:{epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
